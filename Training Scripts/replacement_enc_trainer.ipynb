{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_number(x):\n",
    "    x = x.lower()\n",
    "    new = \"\"\n",
    "    for each in x :\n",
    "        new = new + str( ord(each) )\n",
    "    return new\n",
    "\n",
    "\n",
    "def char_number(x):    \n",
    "    return str( ord(x)  )\n",
    "\n",
    "\n",
    "\n",
    "def getConvertedFormatofText(x):\n",
    "\n",
    "    count_vect = CountVectorizer()\n",
    "    X_train_counts = count_vect.fit_transform(x.values.astype('U'))\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "    return tfidf_transformer\n",
    "\n",
    "\n",
    "def getValidationData(xtest, ytest):\n",
    "    xtest, xvalid, ytest, yvalid = train_test_split(xtest, ytest, test_size=0.50, random_state=42)\n",
    "    return xtest, xvalid, ytest, yvalid\n",
    "\n",
    "\n",
    "def getWordSplitData():\n",
    "    data = pd.read_csv(\"word_split_replacement_encoded.tsv\", sep=\"\\t\")\n",
    "    data.columns = [\"words\", \"output\", \"wordlen\"]\n",
    "    #     data = data.dropna()    \n",
    "    x = data.words.map(char_number)\n",
    "    x = x.values.astype('int32')\n",
    "    y = data.output\n",
    "    y = y.values.astype('int64')\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def getWordSplitOHEData():\n",
    "    data = pd.read_csv(\"word_splits_ohe.tsv\", sep=\"\\t\")\n",
    "#     data = data.dropna()\n",
    "    l = [x for x in range(2, data.shape[1])]\n",
    "    x = data.iloc[:,l]\n",
    "    x = x.values.astype('int16')\n",
    "    y = data.output\n",
    "    y = y.values.astype('int64')\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def getTrainTestData():        \n",
    "    x, y = getWordSplitData()\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.40, random_state = 42)\n",
    "    xtest, xvalid, ytest, yvalid = getValidationData(xtest, ytest)\n",
    "    return  xtrain, xtest, xvalid, ytrain, ytest, yvalid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getTheNetwork(num=6):\n",
    "    i = 0\n",
    "    list_dense = []    \n",
    "    list_dense.append(layers.Dense(units= 100 * (num) , activation='relu', input_dim=1))\n",
    "#     list_dense.append(layers.Dropout(0.3))\n",
    "    for x in range(2,num):\n",
    "        i = i+1\n",
    "        list_dense.append(layers.Dense(units= 100 * ((num)-i), activation='relu' ))        \n",
    "#         list_dense.append(layers.Dropout(0.3))\n",
    "        \n",
    "    \n",
    "    list_dense.append(layers.Dense(units= 100, activation='relu' ))\n",
    "    list_dense.append(layers.Dense(units=1))\n",
    "\n",
    "    return  list_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(getTheNetwork(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 600)               1200      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               300500    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 400)               200400    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               120300    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 200)               60200     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 702,801\n",
      "Trainable params: 702,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss = 'huber', metrics=['mse', 'mae', 'mape'])\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    min_delta=0.01,\n",
    "    patience=40,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, xvalid, ytrain, ytest, yvalid = getTrainTestData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([116, 111,  98, 107, 109], dtype=int32),\n",
       " array([2070, 5120, 2250, 6160, 9140]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain[0:5], ytrain[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_path_keras = \"replacement_file_hacker.h5\"\n",
    "save_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    export_path_keras, monitor='loss', verbose=1, save_best_only=True,\n",
    "    save_weights_only=False, mode='min', save_freq='epoch'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "18801/18801 [==============================] - 218s 12ms/step - loss: 1833.4163 - mse: 6500969.6586 - mae: 1833.9213 - mape: 50.7040 - val_loss: 469.2106 - val_mse: 1035076.5000 - val_mae: 469.7116 - val_mape: 15.3532\n",
      "\n",
      "Epoch 00001: loss improved from inf to 1191.03882, saving model to replacement_file_hacker.h5\n",
      "Epoch 2/100\n",
      "18801/18801 [==============================] - 209s 11ms/step - loss: 449.5110 - mse: 993985.0129 - mae: 450.0098 - mape: 15.2643 - val_loss: 393.3164 - val_mse: 1101740.5000 - val_mae: 393.8153 - val_mape: 16.1154\n",
      "\n",
      "Epoch 00002: loss improved from 1191.03882 to 430.04779, saving model to replacement_file_hacker.h5\n",
      "Epoch 3/100\n",
      " 1555/18801 [=>............................] - ETA: 3:00 - loss: 450.1436 - mse: 1193953.8729 - mae: 450.6422 - mape: 17.1643"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(\n",
    "    xtrain, ytrain,\n",
    "    validation_data=(xvalid, yvalid),\n",
    "    batch_size=100,\n",
    "    epochs=100,\n",
    "    callbacks=[early_stopping, save_checkpoint],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_model('replacement_file_hacker.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = model.predict(xtest)\n",
    "\n",
    "mean_absolute_error(ytest, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[109.263245]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred = model.predict([[2070][5120]])\n",
    "\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[5:, ['loss', 'val_loss']].plot()\n",
    "history_df.loc[5:, ['mae', 'val_mae']].plot()\n",
    "\n",
    "print(history_df['val_loss'].min())\n",
    "print(history_df['val_mae'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n"
     ]
    }
   ],
   "source": [
    "#convert a word to single char cell\n",
    "\n",
    "def writeRow(char, enc, wlen):\n",
    "     with open(\"word_split_replacement_encoded.tsv\", 'a') as decodeMsgWritter:\n",
    "        decodeMsgWritter.write(\"\\n{0}\\t{1}\\t{2}\".format(char,enc,wlen))\n",
    "\n",
    "\n",
    "def convertDatato_chars_encryption(x):\n",
    "    row_x = x.words; row_y = x.output;\n",
    "#     row_x = list(row_x.lower())\n",
    "    for each in range(4,len(row_y)+4,4):\n",
    "        if each-4 == 0:\n",
    "            writeRow(row_x[int(each/4)-1],row_y[each-4:each], len(row_x) )            \n",
    "        else:\n",
    "            writeRow(row_x[int(each/4)-1],row_y[each-4:each], 0 )\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "data = pd.read_csv(\"title_encoded.tsv\", sep=\"\\t\")\n",
    "data.columns=[\"words\",\"output\"]\n",
    "data = data.dropna()\n",
    "\n",
    "\n",
    "data.apply(convertDatato_chars_encryption, axis = 1)\n",
    "print('completed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_latest_p37",
   "language": "python",
   "name": "conda_mxnet_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
